{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we calculate the data of a wind farm with 67 turbines in a time series containing 8000 uniform inflow states.\n",
    "\n",
    "The required imports are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "import foxes\n",
    "import foxes.variables as FV\n",
    "from foxes.utils.runners import DaskRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create the `model book`, adding the turbine type model (see examples above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbook = foxes.ModelBook()\n",
    "mbook.turbine_types[\"NREL5\"] = foxes.models.turbine_types.PCtFile(\n",
    "    \"NREL-5MW-D126-H90.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the `states`. The `data_source` can be any csv-type file (or `pandas` readable equivalent), or a `pandas.DataFrame` object. If it is a file path, then it will first be searched in the file system, and if not found, in the static data. If it is also not found there, an error showing the available static data file names is displayed.\n",
    "\n",
    "In this example the static data file `timeseries_8000.csv.gz` will be used, with content\n",
    "```\n",
    "Time,ws,wd,ti\n",
    "2017-01-01 00:00:00,15.62,244.06,0.0504\n",
    "2017-01-01 00:30:00,15.99,243.03,0.0514\n",
    "2017-01-01 01:00:00,16.31,243.01,0.0522\n",
    "2017-01-01 01:30:00,16.33,241.26,0.0523\n",
    "...\n",
    "```\n",
    "Notice the column names, and how they appear in the `foxes.Timeseries` constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = foxes.input.states.Timeseries(\n",
    "    data_source=\"timeseries_8000.csv.gz\",\n",
    "    output_vars=[FV.WS, FV.WD, FV.TI, FV.RHO],\n",
    "    var2col={FV.WS: \"ws\", FV.WD: \"wd\", FV.TI: \"ti\"},\n",
    "    fixed_vars={FV.RHO: 1.225},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we create the example wind farm with 67 turbines from static data. The file `test_farm_67.csv` has the following structure:\n",
    "```\n",
    "index,label,x,y\n",
    "0,T0,101872.70,1004753.57\n",
    "1,T1,103659.97,1002993.29\n",
    "2,T2,100780.09,1000779.97\n",
    "3,T3,100290.42,1004330.88\n",
    "...\n",
    "```\n",
    "For more options, check the API section `foxes.input.farm_layout`. \n",
    "\n",
    "We consider two turbine models in this example: the wind turbine type `NREL5` from above, and the model `kTI_02` from the `model book`. This model adds the variable `k` for each state and turbine, calculated as `k = kTI * TI`, with constant `kTI = 0.2`. The parameter `k` will later be used by the wake model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm = foxes.WindFarm()\n",
    "foxes.input.farm_layout.add_from_file(\n",
    "    farm, \"test_farm_67.csv\", turbine_models=[\"kTI_02\", \"NREL5\"], verbosity=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the `algorithm`, with further model selections. In particular, two wake models are invoked, the model `Bastankhah_linear` for wind speed deficits and the model `CrespoHernandez_quadratic` for turbulence intensity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = foxes.algorithms.Downwind(\n",
    "    mbook,\n",
    "    farm,\n",
    "    states=states,\n",
    "    rotor_model=\"centre\",\n",
    "    wake_models=[\"Bastankhah_quadratic\", \"CrespoHernandez_max\"],\n",
    "    wake_frame=\"rotor_wd\",\n",
    "    partial_wakes_model=\"auto\",\n",
    "    chunks={FV.STATE: 1000},\n",
    "    verbosity=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice the `chunks` parameter, specifying that always 1000 states should be considered in vectorized form during calculations. The progress can be visualized using `dask`'s `ProgressBar`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    farm_results = algo.calc_farm()\n",
    "\n",
    "fr = farm_results.to_dataframe()\n",
    "print(\"\\n\", fr[[FV.WD, FV.AMB_REWS, FV.REWS, FV.AMB_P, FV.P]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fun of it, we can also run this example in parallel, on a local cluster. Depending on the system and the problem size, this is not neccessarily faster than the above implicitely used default dask scheduler, and it comes with overhead. But for complex calculations it is extremely useful and can really save the day. Read the [docs](https://docs.dask.org/en/stable/deploying.html) for more details and parameters. The following invokes the default settings for the local cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DaskRunner(scheduler=\"distributed\") as runner:\n",
    "    farm_results = runner.run(algo.calc_farm)\n",
    "\n",
    "fr = farm_results.to_dataframe()\n",
    "print(\"\\n\", fr[[FV.WD, FV.AMB_REWS, FV.REWS, FV.AMB_P, FV.P]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `Dashboard` link, which is only valid during runtime and in this case is a `localhost` address. The dashboard gives plenty of information of the progress during the run and is a very useful tool provided by dask.\n",
    "\n",
    "Note that the `DaskRunner` includes an option for the above mentioned `ProgressBar` (cf. API), so the function that is called within `runner.run(...)` should not also try to invoke it as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('foxes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf4a074b666afd0c34fcf4a3f6dfe4d0dd717bc317d9d9cf4935c66389b29031"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
